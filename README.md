# üí¨ Chatbot IA G√©n√©rative avec Recherche Web ‚Äì Partie 1 ‚Äì Test Technique Juillet 2025

## Partie 1 ‚Äì D√©veloppement d‚Äôun Chatbot avec acc√®s Internet

L‚Äôobjectif de cette premi√®re partie √©tait de concevoir un chatbot capable d‚Äôinterroger un mod√®le de langage (LLM) tout en int√©grant des r√©sultats de recherche web r√©cents afin de fournir des r√©ponses enrichies, pertinentes et actualis√©es.

> Requ√™te test √† impl√©menter :
> *‚ÄúQuels sont les derniers d√©veloppements en IA g√©n√©rative annonc√©s cette semaine ? Donne-moi 3 exemples concrets avec leurs sources.‚Äù*

Le projet inclut :
- Une API REST backend construite avec **FastAPI**
- Une logique de **recherche web automatique via SerpAPI**
- Deux modes de g√©n√©ration :
  - Un mod√®le cloud via **OpenRouter**
  - Un mod√®le local via **Hugging Face Transformers**

---

##  Stack technique

| Composant      | Outil utilis√©                                       |
|----------------|-----------------------------------------------------|
| **Langage**    | Python 3.10+                                        |
| **Framework API** | FastAPI + Uvicorn                              |
| **Recherche Web** | `SerpAPI`                                       |
| **LLM Cloud**  | `qwen/qwen2.5-vl-32b-instruct:free`, puis `openchat/openchat-3.5` |
| **LLM Local**  | `microsoft/phi-2` via HuggingFace Transformers     |
| **Frontend**   | React (Create React App)

---

##  Fonctionnalit√©s principales

-  **Recherche web en direct via SerpAPI**
-  **Deux modes de g√©n√©ration** :
  - `/ask` ‚Üí appel au LLM via OpenRouter (cloud)
  - `/ask-local` ‚Üí appel √† un LLM local ex√©cut√© en Python
-  **Prompt enrichi** : les r√©sultats web sont inject√©s dans le contexte du mod√®le
-  **S√©curisation** des cl√©s API avec `.env`
-  **Interface Swagger** pour tester facilement l‚ÄôAPI
-  **Interface React** fonctionnelle c√¥t√© utilisateur

---

##  Exemple de fonctionnement

Requ√™te envoy√©e :
```json
{
  "prompt": "Quels sont les derniers d√©veloppements en IA g√©n√©rative cette semaine ?"
}
```

Processus :
1. Recherche web automatique (SerpAPI)
2. R√©sum√© des 3 meilleurs r√©sultats (titre, URL, extrait)
3. Construction d‚Äôun prompt enrichi
4. Envoi au mod√®le (cloud ou local)
5. R√©ponse g√©n√©r√©e et retourn√©e √† l‚Äôutilisateur

---

##  Structure du projet

```
backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ openrouter_service.py
‚îÇ   ‚îú‚îÄ‚îÄ local_model_service.py
‚îÇ   ‚îî‚îÄ‚îÄ web_search.py
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt

frontend/
‚îú‚îÄ‚îÄ public/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îî‚îÄ‚îÄ index.js
```

---

##  Installation backend

```bash
git clone https://github.com/ton-pseudo/ton-repo.git
cd ton-repo/backend
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate sous Windows
pip install -r requirements.txt
```

###  Ajouter le fichier `.env`

```env
OPENROUTER_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
SERPAPI_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

###  Lancer le backend

```bash
uvicorn app.main:app --reload
```

Swagger disponible sur : http://127.0.0.1:8000/docs

---

##  Lancer le frontend

```bash
cd ../frontend
npm install
npm start
```

Accessible sur : http://localhost:3000

---

##  Fonctionnement interne

1. Appel √† `search_web(prompt)` via SerpAPI
2. R√©sultats inject√©s dans un prompt enrichi
3. Envoi au mod√®le (OpenRouter ou local)
4. G√©n√©ration de r√©ponse bas√©e sur les sources

---

##  Exemple de prompt g√©n√©r√©

```
Voici des informations trouv√©es en ligne :

[1] OpenAI annonce GPT-5 - https://openai.com/gpt5
OpenAI pr√©voit de sortir GPT-5 avec une capacit√© multimodale am√©lior√©e...

[2] Google lance Gemini 1.5 - https://blog.google/ai/gemini
Le mod√®le Gemini introduit une architecture plus modulaire...

[3] Stability AI sort Stable LM 2 - https://stability.ai/news
StableLM 2 permet un meilleur contr√¥le g√©n√©rationnel...

√Ä partir de ces sources, r√©ponds √† la question suivante de mani√®re synth√©tique et pr√©cise :
Quels sont les derniers d√©veloppements en IA g√©n√©rative cette semaine ?
```

## Partie 2 ‚Äì Questions th√©oriques sur le chatbot avec acc√®s internet

Dans le cadre de ce projet, je propose un plan de d√©ploiement sur Azure con√ßu pour r√©pondre aux exigences d‚Äôun grand groupe en mati√®re de scalabilit√©, de s√©curit√©, de fiabilit√© et de maintenabilit√©. L‚Äôobjectif est d‚Äôanticiper une mise en production r√©aliste et professionnelle, en structurant chaque √©tape du d√©ploiement et en justifiant les choix techniques retenus.

La solution d√©velopp√©e repose sur deux composants bien distincts. D‚Äôune part, un backend en Python (FastAPI) charg√© de la logique m√©tier, des appels au LLM via OpenRouter, et de la recherche web via une API comme SerpAPI ou DuckDuckGo. D‚Äôautre part, un frontend en React destin√© √† fournir √† l‚Äôutilisateur une interface claire et ergonomique pour interagir avec le chatbot.

Je recommande une s√©paration stricte entre le frontend et le backend, car elle garantit deux avantages majeurs. D‚Äôabord, cela permet une scalabilit√© ind√©pendante de chaque composant : en cas de forte charge sur l‚ÄôAPI (par exemple un pic de requ√™tes vers OpenRouter), seul le backend devra √™tre renforc√©. Ensuite, cela assure une s√©paration nette des responsabilit√©s : le frontend se concentre sur l‚Äôaffichage et l‚Äôexp√©rience utilisateur, tandis que le backend g√®re les traitements, les appels API, les erreurs et la s√©curit√©. Cette architecture modulaire facilite √©galement les mises √† jour, les tests et la maintenance.

Pour h√©berger cette solution sur Azure, je pr√©conise la cr√©ation des ressources suivantes :

1. **Azure App Service** (Plan Standard S1) pour le backend Python. Ce service permet un d√©ploiement simple, une mont√©e en charge automatique, une int√©gration native avec Key Vault et Application Insights, et offre une disponibilit√© de 99,95 %. Le plan gratuit ne propose pas ces fonctionnalit√©s critiques. Le plan S1 constitue donc un bon compromis entre performance et co√ªt, estim√© √† 77,939 euros par mois.

2. **Azure Static Web Apps** (Plan Standard) pour le frontend React. Ce service fournit un h√©bergement statique performant, via un CDN global, avec d√©ploiement automatis√© depuis GitHub, certificat SSL int√©gr√© et compatibilit√© avec les domaines personnalis√©s. Le plan Standard est adapt√© √† une diffusion fiable et s√©curis√©e, avec un co√ªt de 84,558 euros par mois.

3. **Azure Key Vault** (niveau Standard) pour stocker de mani√®re s√©curis√©e les secrets, tels que les cl√©s API utilis√©es par le backend. L'acc√®s aux secrets s‚Äôeffectue via une **Managed Identity** configur√©e sur App Service. Cette m√©thode permet d‚Äô√©viter tout mot de passe ou cl√© dans le code source. √Ä raison de deux lectures de secret par requ√™te (OpenRouter et SerpAPI), avec une hypoth√®se de 2 000 requ√™tes par jour, on atteint 4 000 lectures par jour, soit 120 000 lectures par mois. Le tarif √©tant de 0,026 ‚Ç¨ pour 10 000 op√©rations, le co√ªt mensuel est de 0,312 ‚Ç¨.

4. **Azure Application Insights** pour le monitoring du backend. Ce service permet de collecter automatiquement les erreurs, les exceptions, les temps de r√©ponse et les journaux d‚Äôactivit√©. En moyenne, chaque requ√™te g√©n√®re une quinzaine d‚Äô√©v√©nements (appels API, logs d‚Äôentr√©e/sortie, erreurs potentielles, etc.). Avec 2 000 requ√™tes par jour, cela repr√©sente environ 30 000 √©v√©nements quotidiens, soit entre 0,5 et 0,9 Go de donn√©es par mois. Le quota gratuit de 5 Go/mois permet de couvrir ces besoins sans surco√ªt.

En ce qui concerne la gestion des acc√®s, je propose de suivre une logique de moindre privil√®ge. Les r√¥les ‚ÄúContributor‚Äù seraient affect√©s aux d√©veloppeurs sur le groupe de ressources Azure, tandis que l‚Äôacc√®s aux secrets serait restreint via un r√¥le ‚ÄúKey Vault Reader‚Äù attribu√© √† l‚ÄôApp Service via une identit√© manag√©e. GitHub Actions disposerait d‚Äôun acc√®s d√©l√©gu√© √† App Service et Static Web Apps via un service principal s√©curis√© ou OpenID Connect, pour automatiser les d√©ploiements sans manipulation manuelle de cl√©s.

La mise en production serait automatis√©e via GitHub Actions. Le pipeline du backend inclurait une √©tape de **linting** avec *flake8*, afin de garantir la conformit√© du code aux standards Python (structure, lisibilit√©, conventions). Ensuite, les **tests unitaires** seraient ex√©cut√©s via *pytest*, pour valider les comportements critiques (gestion des erreurs, coh√©rence des r√©ponses, etc.). En cas de succ√®s, le d√©ploiement serait automatiquement d√©clench√© vers Azure App Service. Le frontend suivrait un pipeline similaire : build React puis d√©ploiement sur Static Web Apps.

Pour garantir la stabilit√© de l‚Äôapplication en production, Application Insights assurerait une supervision continue. Des alertes seraient configur√©es pour remonter les anomalies critiques (erreurs 5xx, lenteurs, exceptions non g√©r√©es). La gestion des erreurs serait centralis√©e dans le backend : chaque appel critique serait encapsul√© dans un bloc `try/except`, permettant de logguer les incidents (type d‚Äôexception, code retour, temps √©coul√©, etc.), tout en renvoyant un message utilisateur clair.

Enfin, une strat√©gie de sauvegarde et de reprise serait mise en place. Le code est versionn√© sur GitHub, les secrets sont centralis√©s dans Key Vault, et la configuration Azure est exportable via CLI. En cas de besoin, une restauration rapide serait possible √† partir de scripts Terraform ou ARM. Les journaux pourraient √©galement √™tre archiv√©s dans Azure Blob Storage pour conservation longue dur√©e.

Cette architecture repose sur des composants d√©coupl√©s, maintenables et √©volutifs. Elle offre une s√©paration claire des responsabilit√©s, une s√©curit√© robuste, une supervision fiable et un d√©ploiement enti√®rement automatis√©. Le tout pour un co√ªt total estim√© √† **162,809 euros par mois**, ce qui reste coh√©rent et justifi√© pour un d√©ploiement professionnel au sein d‚Äôun grand groupe.


## Partie 3 ‚Äì Vision Language Models

**Remarque** : La partie technique des Vision Language Models n‚Äôa pas pu √™tre r√©alis√©e faute de mat√©riel compatible (GPU avec au moins 8 Go de VRAM). Cette section pr√©sente donc uniquement l‚Äôapproche th√©orique d√©taill√©e.

### 1. Extraction multimodale

Le pipeline commence par une √©tape d‚Äôextraction o√π l‚Äôon identifie les blocs de texte ainsi que les √©l√©ments visuels du document. Chaque image est associ√©e √† son paragraphe ou √† sa l√©gende la plus proche en utilisant des r√®gles de proximit√© et de structure (par exemple, une image ins√©r√©e entre deux paragraphes sera li√©e au paragraphe pr√©c√©dent). Ce couplage image + texte est indispensable pour la compr√©hension contextuelle.

### 2. Encodage avec un VLM

Chaque couple image + texte est ensuite envoy√© √† un **Vision Language Model** (par exemple : Qwen-VL, BLIP-2, Kosmos-2). Ce mod√®le prend les deux modalit√©s en entr√©e et g√©n√®re un **embedding unifi√©**, c‚Äôest-√†-dire une repr√©sentation vectorielle unique qui capture √† la fois le contenu visuel, le contenu textuel, et surtout leur lien s√©mantique.

Contrairement √† une approche qui vectorise s√©par√©ment texte et image, puis fusionne les deux vecteurs, l‚Äôencodage unifi√© permet d‚Äôobtenir directement une repr√©sentation coh√©rente et int√©gr√©e.

### 3. Indexation vectorielle

Les vecteurs produits par le VLM sont stock√©s dans une base vectorielle (telle que FAISS, Qdrant ou Weaviate). Chaque vecteur est enrichi de m√©tadonn√©es (page, source, type de contenu, etc.) permettant une recherche plus fine. Les documents sont d√©coup√©s en chunks multimodaux pour garantir la granularit√© des recherches.

### 4. Recherche et g√©n√©ration (RAG)

Lorsqu‚Äôun utilisateur pose une question en langage naturel, celle-ci est encod√©e avec le **m√™me VLM** que celui utilis√© pour les documents. La base vectorielle est interrog√©e par similarit√© pour identifier les chunks multimodaux les plus proches s√©mantiquement.

Les r√©sultats sont ensuite concat√©n√©s et transmis comme **contexte** √† un **mod√®le g√©n√©ratif** (par exemple GPT, Mistral, Qwen), qui peut alors produire une r√©ponse ancr√©e dans les documents, en tenant compte √† la fois du texte et des √©l√©ments visuels.

## Avantages de l‚Äôapproche

- **Coh√©rence s√©mantique** : l‚Äôencodage unifi√© garantit que les relations entre texte et image sont conserv√©es d√®s l‚Äôentr√©e du mod√®le.
- **Simplicit√© du pipeline** : un seul mod√®le est utilis√© pour encoder les documents et les requ√™tes.
- **Pertinence des r√©ponses** : le LLM acc√®de √† des contextes riches et structur√©s, m√™me lorsqu‚Äôils reposent sur des √©l√©ments visuels.
- **Scalabilit√©** : les vecteurs peuvent √™tre pr√©calcul√©s et stock√©s, et chaque √©tape du pipeline peut √™tre containeris√©e ou orchestr√©e s√©par√©ment.
