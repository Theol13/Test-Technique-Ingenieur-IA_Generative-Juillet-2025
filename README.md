# ğŸ’¬ Chatbot IA GÃ©nÃ©rative avec Recherche Web â€“ Partie 1 â€“ Test Technique Juillet 2025

## Partie 1 â€“ DÃ©veloppement dâ€™un Chatbot avec accÃ¨s Internet

Lâ€™objectif de cette premiÃ¨re partie Ã©tait de concevoir un chatbot capable dâ€™interroger un modÃ¨le de langage (LLM) tout en intÃ©grant des rÃ©sultats de recherche web rÃ©cents afin de fournir des rÃ©ponses enrichies, pertinentes et actualisÃ©es.

> RequÃªte test Ã  implÃ©menter :
> *â€œQuels sont les derniers dÃ©veloppements en IA gÃ©nÃ©rative annoncÃ©s cette semaine ? Donne-moi 3 exemples concrets avec leurs sources.â€*

Le projet inclut :
- Une API REST backend construite avec **FastAPI**
- Une logique de **recherche web automatique via SerpAPI**
- Deux modes de gÃ©nÃ©ration :
  - Un modÃ¨le cloud via **OpenRouter**
  - Un modÃ¨le local via **Hugging Face Transformers**

---

## ğŸ§± Stack technique

| Composant      | Outil utilisÃ©                                       |
|----------------|-----------------------------------------------------|
| **Langage**    | Python 3.10+                                        |
| **Framework API** | FastAPI + Uvicorn                              |
| **Recherche Web** | `SerpAPI`                                       |
| **LLM Cloud**  | `qwen/qwen2.5-vl-32b-instruct:free`, puis `openchat/openchat-3.5` |
| **LLM Local**  | `microsoft/phi-2` via HuggingFace Transformers     |
| **Frontend**   | React (Create React App)

---

## ğŸš€ FonctionnalitÃ©s principales

- ğŸ” **Recherche web en direct via SerpAPI**
- ğŸ¤– **Deux modes de gÃ©nÃ©ration** :
  - `/ask` â†’ appel au LLM via OpenRouter (cloud)
  - `/ask-local` â†’ appel Ã  un LLM local exÃ©cutÃ© en Python
- ğŸ§  **Prompt enrichi** : les rÃ©sultats web sont injectÃ©s dans le contexte du modÃ¨le
- ğŸ” **SÃ©curisation** des clÃ©s API avec `.env`
- ğŸ§ª **Interface Swagger** pour tester facilement lâ€™API
- ğŸ–¥ï¸ **Interface React** fonctionnelle cÃ´tÃ© utilisateur

---

## ğŸ§ª Exemple de fonctionnement

RequÃªte envoyÃ©e :
```json
{
  "prompt": "Quels sont les derniers dÃ©veloppements en IA gÃ©nÃ©rative cette semaine ?"
}
```

Processus :
1. Recherche web automatique (SerpAPI)
2. RÃ©sumÃ© des 3 meilleurs rÃ©sultats (titre, URL, extrait)
3. Construction dâ€™un prompt enrichi
4. Envoi au modÃ¨le (cloud ou local)
5. RÃ©ponse gÃ©nÃ©rÃ©e et retournÃ©e Ã  lâ€™utilisateur

---

## ğŸ“‚ Structure du projet

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ openrouter_service.py
â”‚   â”œâ”€â”€ local_model_service.py
â”‚   â””â”€â”€ web_search.py
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt

frontend/
â”œâ”€â”€ public/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx
â”‚   â””â”€â”€ index.js
```

---

## âš™ï¸ Installation backend

```bash
git clone https://github.com/ton-pseudo/ton-repo.git
cd ton-repo/backend
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate sous Windows
pip install -r requirements.txt
```

### ğŸ” Ajouter le fichier `.env`

```env
OPENROUTER_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
SERPAPI_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### â–¶ï¸ Lancer le backend

```bash
uvicorn app.main:app --reload
```

Swagger disponible sur : http://127.0.0.1:8000/docs

---

## âš™ï¸ Lancer le frontend

```bash
cd ../frontend
npm install
npm start
```

Accessible sur : http://localhost:3000

---

## ğŸ” Fonctionnement interne

1. Appel Ã  `search_web(prompt)` via SerpAPI
2. RÃ©sultats injectÃ©s dans un prompt enrichi
3. Envoi au modÃ¨le (OpenRouter ou local)
4. GÃ©nÃ©ration de rÃ©ponse basÃ©e sur les sources

---

## ğŸ“Œ Exemple de prompt gÃ©nÃ©rÃ©

```
Voici des informations trouvÃ©es en ligne :

[1] OpenAI annonce GPT-5 - https://openai.com/gpt5
OpenAI prÃ©voit de sortir GPT-5 avec une capacitÃ© multimodale amÃ©liorÃ©e...

[2] Google lance Gemini 1.5 - https://blog.google/ai/gemini
Le modÃ¨le Gemini introduit une architecture plus modulaire...

[3] Stability AI sort Stable LM 2 - https://stability.ai/news
StableLM 2 permet un meilleur contrÃ´le gÃ©nÃ©rationnel...

Ã€ partir de ces sources, rÃ©ponds Ã  la question suivante de maniÃ¨re synthÃ©tique et prÃ©cise :
Quels sont les derniers dÃ©veloppements en IA gÃ©nÃ©rative cette semaine ?
```

## Partie 2 â€“ Questions thÃ©oriques sur le chatbot avec accÃ¨s internet

Dans le cadre de ce projet, je propose un plan de dÃ©ploiement sur Azure conÃ§u pour rÃ©pondre aux exigences dâ€™un grand groupe en matiÃ¨re de scalabilitÃ©, de sÃ©curitÃ©, de fiabilitÃ© et de maintenabilitÃ©. Lâ€™objectif est dâ€™anticiper une mise en production rÃ©aliste et professionnelle, en structurant chaque Ã©tape du dÃ©ploiement et en justifiant les choix techniques retenus.

La solution dÃ©veloppÃ©e repose sur deux composants bien distincts. Dâ€™une part, un backend en Python (FastAPI) chargÃ© de la logique mÃ©tier, des appels au LLM via OpenRouter, et de la recherche web via une API comme SerpAPI ou DuckDuckGo. Dâ€™autre part, un frontend en React destinÃ© Ã  fournir Ã  lâ€™utilisateur une interface claire et ergonomique pour interagir avec le chatbot.

Je recommande une sÃ©paration stricte entre le frontend et le backend, car elle garantit deux avantages majeurs. Dâ€™abord, cela permet une scalabilitÃ© indÃ©pendante de chaque composant : en cas de forte charge sur lâ€™API (par exemple un pic de requÃªtes vers OpenRouter), seul le backend devra Ãªtre renforcÃ©. Ensuite, cela assure une sÃ©paration nette des responsabilitÃ©s : le frontend se concentre sur lâ€™affichage et lâ€™expÃ©rience utilisateur, tandis que le backend gÃ¨re les traitements, les appels API, les erreurs et la sÃ©curitÃ©. Cette architecture modulaire facilite Ã©galement les mises Ã  jour, les tests et la maintenance.

Pour hÃ©berger cette solution sur Azure, je prÃ©conise la crÃ©ation des ressources suivantes :

1. **Azure App Service** (Plan Standard S1) pour le backend Python. Ce service permet un dÃ©ploiement simple, une montÃ©e en charge automatique, une intÃ©gration native avec Key Vault et Application Insights, et offre une disponibilitÃ© de 99,95 %. Le plan gratuit ne propose pas ces fonctionnalitÃ©s critiques. Le plan S1 constitue donc un bon compromis entre performance et coÃ»t, estimÃ© Ã  77,939 euros par mois.

2. **Azure Static Web Apps** (Plan Standard) pour le frontend React. Ce service fournit un hÃ©bergement statique performant, via un CDN global, avec dÃ©ploiement automatisÃ© depuis GitHub, certificat SSL intÃ©grÃ© et compatibilitÃ© avec les domaines personnalisÃ©s. Le plan Standard est adaptÃ© Ã  une diffusion fiable et sÃ©curisÃ©e, avec un coÃ»t de 84,558 euros par mois.

3. **Azure Key Vault** (niveau Standard) pour stocker de maniÃ¨re sÃ©curisÃ©e les secrets, tels que les clÃ©s API utilisÃ©es par le backend. L'accÃ¨s aux secrets sâ€™effectue via une **Managed Identity** configurÃ©e sur App Service. Cette mÃ©thode permet dâ€™Ã©viter tout mot de passe ou clÃ© dans le code source. Ã€ raison de deux lectures de secret par requÃªte (OpenRouter et SerpAPI), avec une hypothÃ¨se de 2 000 requÃªtes par jour, on atteint 4 000 lectures par jour, soit 120 000 lectures par mois. Le tarif Ã©tant de 0,026 â‚¬ pour 10 000 opÃ©rations, le coÃ»t mensuel est de 0,312 â‚¬.

4. **Azure Application Insights** pour le monitoring du backend. Ce service permet de collecter automatiquement les erreurs, les exceptions, les temps de rÃ©ponse et les journaux dâ€™activitÃ©. En moyenne, chaque requÃªte gÃ©nÃ¨re une quinzaine dâ€™Ã©vÃ©nements (appels API, logs dâ€™entrÃ©e/sortie, erreurs potentielles, etc.). Avec 2 000 requÃªtes par jour, cela reprÃ©sente environ 30 000 Ã©vÃ©nements quotidiens, soit entre 0,5 et 0,9 Go de donnÃ©es par mois. Le quota gratuit de 5 Go/mois permet de couvrir ces besoins sans surcoÃ»t.

En ce qui concerne la gestion des accÃ¨s, je propose de suivre une logique de moindre privilÃ¨ge. Les rÃ´les â€œContributorâ€ seraient affectÃ©s aux dÃ©veloppeurs sur le groupe de ressources Azure, tandis que lâ€™accÃ¨s aux secrets serait restreint via un rÃ´le â€œKey Vault Readerâ€ attribuÃ© Ã  lâ€™App Service via une identitÃ© managÃ©e. GitHub Actions disposerait dâ€™un accÃ¨s dÃ©lÃ©guÃ© Ã  App Service et Static Web Apps via un service principal sÃ©curisÃ© ou OpenID Connect, pour automatiser les dÃ©ploiements sans manipulation manuelle de clÃ©s.

La mise en production serait automatisÃ©e via GitHub Actions. Le pipeline du backend inclurait une Ã©tape de **linting** avec *flake8*, afin de garantir la conformitÃ© du code aux standards Python (structure, lisibilitÃ©, conventions). Ensuite, les **tests unitaires** seraient exÃ©cutÃ©s via *pytest*, pour valider les comportements critiques (gestion des erreurs, cohÃ©rence des rÃ©ponses, etc.). En cas de succÃ¨s, le dÃ©ploiement serait automatiquement dÃ©clenchÃ© vers Azure App Service. Le frontend suivrait un pipeline similaire : build React puis dÃ©ploiement sur Static Web Apps.

Pour garantir la stabilitÃ© de lâ€™application en production, Application Insights assurerait une supervision continue. Des alertes seraient configurÃ©es pour remonter les anomalies critiques (erreurs 5xx, lenteurs, exceptions non gÃ©rÃ©es). La gestion des erreurs serait centralisÃ©e dans le backend : chaque appel critique serait encapsulÃ© dans un bloc `try/except`, permettant de logguer les incidents (type dâ€™exception, code retour, temps Ã©coulÃ©, etc.), tout en renvoyant un message utilisateur clair.

Enfin, une stratÃ©gie de sauvegarde et de reprise serait mise en place. Le code est versionnÃ© sur GitHub, les secrets sont centralisÃ©s dans Key Vault, et la configuration Azure est exportable via CLI. En cas de besoin, une restauration rapide serait possible Ã  partir de scripts Terraform ou ARM. Les journaux pourraient Ã©galement Ãªtre archivÃ©s dans Azure Blob Storage pour conservation longue durÃ©e.

Cette architecture repose sur des composants dÃ©couplÃ©s, maintenables et Ã©volutifs. Elle offre une sÃ©paration claire des responsabilitÃ©s, une sÃ©curitÃ© robuste, une supervision fiable et un dÃ©ploiement entiÃ¨rement automatisÃ©. Le tout pour un coÃ»t total estimÃ© Ã  **162,809 euros par mois**, ce qui reste cohÃ©rent et justifiÃ© pour un dÃ©ploiement professionnel au sein dâ€™un grand groupe.


## Partie 3 â€“ Vision Language Models

**Remarque** : La partie technique des Vision Language Models nâ€™a pas pu Ãªtre rÃ©alisÃ©e faute de matÃ©riel compatible (GPU avec au moins 8 Go de VRAM). Cette section prÃ©sente donc uniquement lâ€™approche thÃ©orique dÃ©taillÃ©e.

### 1. Extraction multimodale

Le pipeline commence par une Ã©tape dâ€™extraction oÃ¹ lâ€™on identifie les blocs de texte ainsi que les Ã©lÃ©ments visuels du document. Chaque image est associÃ©e Ã  son paragraphe ou Ã  sa lÃ©gende la plus proche en utilisant des rÃ¨gles de proximitÃ© et de structure (par exemple, une image insÃ©rÃ©e entre deux paragraphes sera liÃ©e au paragraphe prÃ©cÃ©dent). Ce couplage image + texte est indispensable pour la comprÃ©hension contextuelle.

### 2. Encodage avec un VLM

Chaque couple image + texte est ensuite envoyÃ© Ã  un **Vision Language Model** (par exemple : Qwen-VL, BLIP-2, Kosmos-2). Ce modÃ¨le prend les deux modalitÃ©s en entrÃ©e et gÃ©nÃ¨re un **embedding unifiÃ©**, câ€™est-Ã -dire une reprÃ©sentation vectorielle unique qui capture Ã  la fois le contenu visuel, le contenu textuel, et surtout leur lien sÃ©mantique.

Contrairement Ã  une approche qui vectorise sÃ©parÃ©ment texte et image, puis fusionne les deux vecteurs, lâ€™encodage unifiÃ© permet dâ€™obtenir directement une reprÃ©sentation cohÃ©rente et intÃ©grÃ©e.

### 3. Indexation vectorielle

Les vecteurs produits par le VLM sont stockÃ©s dans une base vectorielle (telle que FAISS, Qdrant ou Weaviate). Chaque vecteur est enrichi de mÃ©tadonnÃ©es (page, source, type de contenu, etc.) permettant une recherche plus fine. Les documents sont dÃ©coupÃ©s en chunks multimodaux pour garantir la granularitÃ© des recherches.

### 4. Recherche et gÃ©nÃ©ration (RAG)

Lorsquâ€™un utilisateur pose une question en langage naturel, celle-ci est encodÃ©e avec le **mÃªme VLM** que celui utilisÃ© pour les documents. La base vectorielle est interrogÃ©e par similaritÃ© pour identifier les chunks multimodaux les plus proches sÃ©mantiquement.

Les rÃ©sultats sont ensuite concatÃ©nÃ©s et transmis comme **contexte** Ã  un **modÃ¨le gÃ©nÃ©ratif** (par exemple GPT, Mistral, Qwen), qui peut alors produire une rÃ©ponse ancrÃ©e dans les documents, en tenant compte Ã  la fois du texte et des Ã©lÃ©ments visuels.

## Avantages de lâ€™approche

- **CohÃ©rence sÃ©mantique** : lâ€™encodage unifiÃ© garantit que les relations entre texte et image sont conservÃ©es dÃ¨s lâ€™entrÃ©e du modÃ¨le.
- **SimplicitÃ© du pipeline** : un seul modÃ¨le est utilisÃ© pour encoder les documents et les requÃªtes.
- **Pertinence des rÃ©ponses** : le LLM accÃ¨de Ã  des contextes riches et structurÃ©s, mÃªme lorsquâ€™ils reposent sur des Ã©lÃ©ments visuels.
- **ScalabilitÃ©** : les vecteurs peuvent Ãªtre prÃ©calculÃ©s et stockÃ©s, et chaque Ã©tape du pipeline peut Ãªtre containerisÃ©e ou orchestrÃ©e sÃ©parÃ©ment.
